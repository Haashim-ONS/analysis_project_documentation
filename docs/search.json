[
  {
    "objectID": "quality_questions.html",
    "href": "quality_questions.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo get the most out of the template, we strongly recommend that teams identify who will take the key quality assurance roles of Commissioner, Senior Responsible Owner and Analytical Assurer and name the analytical team at the start of the analytical cycle. This is crucial because together these roles help to make sure that the analysis you do is fit-for-purpose.",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "quality_questions.html#iv.-delivery",
    "href": "quality_questions.html#iv.-delivery",
    "title": "Quality Questions",
    "section": "IV. Delivery",
    "text": "IV. Delivery\n\nQuality questions and why they matterThe questions and the Code of PracticeLinking the questions to AQuA roles\n\n\n\n\n\n\n\nQuestion\n\n\nWhy do I need to know the answer to this?\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nOften the aim of final output is to inform decison-making. Output might include predictions, involving lots of underlying assumptions. It is critical that you support your users to make appropriate use of outputs and understand what can and cannot be inferred. Without this, users may misinterpret findings, make in appropriate comparisons, use the analysis for unsuitable purposes and arrive at the wrong conclusions. For example, a non-expert user may wrongly interpret correlation as causation or use incomplete or disconnected data to make forecasts.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nYou should describe why limitations related to data and methods exist, why they cannot be overcome using the chosen approach and their impact on the quality and interpretation of the output. Analysis is of very little value if limitations aren’t properly documented and explained.\n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nYou should work with users, experts, and other relevant stakeholders to verify the credibility of outputs and sense check that they are useful.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nOutputs are never 100% accurate. Users need to understand how uncertainties related to data, assumptions and methodology feed into and through the analysis workflow and what this means for the use of the outputs. Results must clearly explain how uncertainty affects the findings from the analysis, or we risk misinterpretations and conclusions being overly reliant on imprecise results.\n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nYou must support your users to understand relevant uncertainties which are not captured in the analysis. When you can, make reasonable judgements about the likely size and direction of unquantified uncertainty. Provide a qualitative description informing users about why the uncertainty cannot be quantified and their likely impact.\n\n\n\n\nQ57\n\n\nIs workflow documentation including technical guides and code repositories publicly available?\n\n\nTransparency about your analysis supports proper scrutiny and challenge, promotes public trust, and encourages re-use of the resources you develop.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nA good technical guide helps everybody to understand what the analysis does and how it works. A well-written technical guide is essential for effective maintenance of the analysis. It helps users of the analysis to replicate the findings, get answers to methodology questions and build their trust in the output.\n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nThe technical guide is complemented by fully documented analysis code. Code documentation must comply with good practice so new users can understand and execute the code as easily and quickly as possible.\n\n\n\n\nQ60\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nExternal critique makes analysis more robust. Users should be able to give feedback to your team to ensure that results meet their needs. User feedback and customer reviews inform you of issues and changes that you might need to make. They also act as evidence that users have been consulted.\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\nWhich pillar and principle of Code of Practice matter here?\n*Trustworthiness (T), Quality (Q), Value (V)\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.Q1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q3.1 Statistics should be produced to a level of quality that meets users’ needs. The strengths and limitations of the statistics and data should be considered in relation to different uses, and clearly explained alongside the statistics.\n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nQ1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported.Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.Q3.3 The extent and nature of any uncertainty in the estimates should be clearly explained.\n\n\n\n\nQ57\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.V3 Statistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.\n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nT4.4 Good business practices should be maintained in the use of resources. Where appropriate, statistics producers should take opportunities to share resources and collaborate to achieve common goals and produce coherent statistics.Q2.1 Methods and processes should be based on national or international good practice, scientific principles, or established professional consensus.\n\n\n\n\nQ60\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nV1 Users of statistics and data should be at the centre of statistical production; their needs should be understood, their views sought and acted upon, and their use of statistics supported. V1.4 Statistics producers should engage publicly through a variety of means that are appropriate to the needs of different audiences and proportionate to the potential of the statistics to serve the public good. An open dialogue should be maintained using proactive formal and informal engagement to listen to the views of new and established contacts. Statistics producers should undertake public engagement collaboratively wherever possible, working in partnership with policy makers and other statistics producers to obtain the views of stakeholders.V1.5 The views received from users, potential users and other stakeholders should be addressed, where practicable. Statistics producers should consider whether to produce new statistics to meet identified information gaps. Feedback should be provided to them about how their needs can and cannot be met, being transparent about reasons for the decisions made and any constraints.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich AQuA role(s) would normally answer this?\n\n\nWhy are these AQuA roles involved?\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nCommissioner, analyst\n\n\nDuring the delivery phase, the commissioner receives the results of the analysis and decides whether it meets their needs. The analyst provides sufficient information to support the commissioner to make an informed decision.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nCommissioner, analytical assurer, analyst\n\n\nThe commissioner must be confident in the quality of the outputs. They should understand the strengths, limitations and context of the analysis so that the results are correctly interpreted. Analytical assurer sign-off provides confidence that analysis risks, limitations and major assumptions are understood by the users of the analysis. Analysts make sure that the commissioner and analytical assurer have the evidence they need.\n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst and analytical assurer should enable and encourage peer review. Peer reviews provide useful critical challenge about the analytical approach, application of methods and interpretation of the analysis. Verification and peer review of work should be done by analysts who had no involvemen in the work  so their views are independent.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nAnalyst, commissioner\n\n\nThe analyst must determine and communicate the uncertainty associated with the analysis so the commissioner can make informed decisions. The commissioner should ensure that an assessment of uncertainty has been provided and that the implications of uncertainty are understood.\n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nAnalyst, commissioner\n\n\nIf uncertainty is too complex to quantify, even approximately, the analysts should explain this so the commissioner can take this into account. In communicating analysis results to decision-makers and stakeholders, the commissioner should be open about the existence of deep uncertainties whose impact cannot be assessed, and explain how they are managed in the analysis.\n\n\n\n\nQ57\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate design documentation. Best practice includes maintaining a record of the analysis workflow in a technical report, including a concept of analysis, user requirements, design specification, functional specification, data dictionary, and test plan. Code should be properly documented.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate documentation. Best practice includes maintaining a record of the work that has been done in a technical report, including a full description of the analysis, user requirements, design specification, functional specification, data dictionary, and test plan. The analytical assurer makes sure that the documentation is fit for purpose.\n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nAnalyst, analytical assurer\n\n\nAnalysts should develop and maintain analysis code in line with best practice. Code must comply with relevant policies and standards.\n\n\n\n\nQ60\n\n\nIs there a clear feedback mechanism so users can report back on the suitability of outputs?\n\n\nAnalyst, senior responsible owner\n\n\nYou can assess the usefulness of the analysis by getting feedback from users, stakeholders and other experts. Quality analysis should be free of prejudice or bias. The SRO and analysts should check that the analysis follows the principles of RIGOUR (Repeatable, Independent, Grounded in reality, Objective, Uncertainty-managed, Robust)",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html",
    "href": "assumptions_and_issues_log.html",
    "title": "Assumptions Log",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#definitions",
    "href": "assumptions_and_issues_log.html#definitions",
    "title": "Assumptions Log",
    "section": "Definitions",
    "text": "Definitions\nStart filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis output report) and financial year of interest.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#definitions-1",
    "href": "assumptions_and_issues_log.html#definitions-1",
    "title": "Assumptions Log",
    "section": "Definitions",
    "text": "Definitions\nStart filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis publications or output report) and financial year of interest.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\n\n\n\n\nFeedback\nWe are keen to understand the challenges faced by analytical teams in ONS when complying with good practice. To evaluate the effectiveness of this guidance, we would be grateful if you could fill in a short survey.\nWe appreciate your feedback. It will help us to make additional tools and resources on good practice for producing quality analysis.\nIf you have questions about this guidance or the survey please email ASAP@ons.gov.uk with “analytical QA” in the subject header.",
    "crumbs": [
      "Feedback"
    ]
  },
  {
    "objectID": "faqs.html",
    "href": "faqs.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#when-should-i-use-this-guidance",
    "href": "faqs.html#when-should-i-use-this-guidance",
    "title": "Frequently Asked Questions",
    "section": "When should I use this guidance?",
    "text": "When should I use this guidance?\nIf you are starting work on a new analysis project we recommend that you use this guidance right from the scoping stage. For teams who are in the middle of an analysis, you can still use the guidance and templates to think about the quality questions from the scoping and design stages and continue recording the answers up until the delivery stage.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "href": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "title": "Frequently Asked Questions",
    "section": "Which analysis is in scope of the guidance?",
    "text": "Which analysis is in scope of the guidance?\nThe guidance applies to all analysis workflows, including analysis to support research, advice for other departments, ad-hoc and one-off outputs and regular outputs.\nRegular outputs often fulfil at least one of the criteria for what makes a business critical model:\n* Underpins essential financial and funding decisions;\n* Essential to the achievement of business plan actions and priorities;\n* Must be error free or else risk serious financial or legal penalties or reputational damage for the organisation.\nBusiness critical analysis requires the highest level of scrutiny and the governance arrangements must be appropriate for the level of risk.\nFor ad-hoc outputs with quick turnarounds and limited time and resources, analysis still needs sufficient quality assurance to ensure it is fit for purpose. Ad-hoc outputs are often used as part of the evidence when making important policy decisions, so getting them right is very important.\nWe strongly encourage senior responsible owners of both regular and ad-hoc analysis to answer all questions relevant to their role.\nCompleting the logs makes it much easier to understand the risks that the analysis carries and will help you plan to mitigate them. Having a proper audit trail and comprehensive logs also help when:\n* You need to explain why the analysis works as it does.\n* You need to raise quality and resourcing issues and push back against demands that are unrealistic.\n* You need to understand and communicate potential risks.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "href": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "title": "Frequently Asked Questions",
    "section": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?",
    "text": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?\nThe AQuA Book sets out four roles responsible for the assurance of analysis:\n* Commissioner\n* Senior Responsible Owner (SRO) (usually leads the analyst team)\n* Analysts (who usually report to the SRO) and\n* Analytical Assurer.\nWhat matters here is that each assurance role is covered in your workflow and that individuals know about and accept their responsibilities, not the name of the role.\nAs a project team, you should make sure that each role is in place and you understand how it will operate for you so that you have the right assurance.\nThe AQuA Book makes no expectations about levels of seniority or grade of each of the occupiers of the roles. It does not specify whether roles should be held by a person or could be held by a committee or other governance group (such as a senior leadership team).\nThe key consideration is whether or not the person or group that undertakes the assurance role have the skills and resources they need to meet the requirements of the role. How the roles are covered in an analysis workflow varies from project to project, depending on how the work is planned, managed and assured.\nWe suggest that if roles are taken by individuals, the SRO and commissioner should usually be at Grade 7 or above. If you are still unsure about how to allocate the roles among your team and governance groups, please email us with “analysis assurance” in the subject header and we will help you to make the decision.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "href": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "title": "Frequently Asked Questions",
    "section": "What help is available to answer the quality questions?",
    "text": "What help is available to answer the quality questions?\nFor ONS staff, the ONS Quality Central wiki contains lots of useful guidance, templates and mandatory training to help you work through and answer the quality questions. It includes the ONS Quality Standard for Analysis.\nThese cross-government resources are also likely to be useful:\n* Government Data Quality Hub Quality Questions and Red Flags and Government Data Quality Framework\n* Office for Statistics Regulation Quality Assurance of Administrative Data toolkit and Quality and statistics: an OSR perspective.\n* The Uncertainty Toolkit for Analysts in Government",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#decisions-log",
    "href": "assumptions_and_issues_log.html#decisions-log",
    "title": "Assumptions Log",
    "section": "Decisions log",
    "text": "Decisions log\nThe decisions log is a template to record all the important decisions that are made during the analysis lifecycle, as well as when they were made, who made them and why. Without documentation, decisions may be understood by one part of the team and unknown by others. The decisions log is there to make sure that everybody knows about the decisions that have been made about the analysis. It provides an audit trail of the decisions that were made.\nDecision ID: Give each decision a unique ID so that it can be tracked easily and cross referenced.\nDecision name: Enter a name for the decision.\nDate of decision: Enter the date the decision was made.\nPlain English description of decision A brief summary of the decision in plain English explaining what it was about and how it applies in the analysis.\nName of person or group signing off the decision: Enter the name of the person or group who made the decision. If the decision was made by a committee, link to the minute where the decision is documented.\nRole of person or group signing off the decision: Provide a short description of the person or group’s role in the project. Decisions are often signed off by the project lead or Senior Responsible Owner, for example.\nDate of last review / update: Date the decision was last considered. This is here so you know when the decision was last looked at. Projects evolve, and decisions might need to be reviewed.\nReviewed by: Name(s) of people or group who last reviewed the decision.\nNext review/update due on: Enter the date the decision next needs to be reviewed or updated.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#issues-log",
    "href": "assumptions_and_issues_log.html#issues-log",
    "title": "Assumptions Log",
    "section": "Issues log",
    "text": "Issues log\nThe issues log provides a standard template to record all the issues incurred during the analysis lifecycle. Without documentation, risks and issues may be well understood by one part of the team and totally unknown by others. The issues log ensures that everybody knows about the issues that the analysis faces. The log helps teams to store problems and challenges for future review and mitigation. It provides an audit trail of past, live and untreated issues.\nIssue ID: Give each issue a unique ID so that it can be tracked easily and cross referenced.\nIssue name: Enter a name for the issue. It could be anything raised by the team, anything raised during quality assurance by the team members or external reviewers. Include the location of the issue, for example, line number of code, error in publication, where it arises in the workflow, resource constraint.\nDate identified: Enter the date the issue was first identified.\nPlain English description of issue: A brief summary of the underlying cause and nature of issue in plain English explaining what is creating problem.\nImpact of issue: Brief summary of how the issue affects the project, for example, timeline, accuracy, cost.\nStatus of issue: From the dropdown menu, select ‘Live’ if the issue is being handled, ‘Resolved’ if the issue has been resolved or ‘Untreated’ if the issue has yet to be handled.\nJustification of status: Brief summary of how the solution has resolved the issue if the status is ‘Resolved’. For ‘Untreated’ issues, explain why the issue will be dealt with later.\nProof of resolution: Write the name of the output report/methodology paper or the published document where the issue is resolved. If the issue relates to code, identify the module and line number.\nDate of last review/update: Enter the date the issue was last reviewed or updated.\nReviewed by: Enter the full name of individual(s) or group who reviewed the issue.\nNext review/update due on: Enter the date the issue next needs to be reviewed or updated.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  },
  {
    "objectID": "assumptions_and_issues_log.html#assumptions-log-1",
    "href": "assumptions_and_issues_log.html#assumptions-log-1",
    "title": "Assumptions Log",
    "section": "Assumptions log",
    "text": "Assumptions log\nAssumption ID: Give each assumption a unique ID so that it can be tracked easily and cross referenced.\nLocation in code, documentation or publication: Write the name of the document or publication where the assumption is applied. If the assumption is applied in your code, identify where the assumption applies by citing the module and line number.\nPlain English description of assumption: Write a clear description of the assumption in plain English. For example, “We assume that our sample of income data is representative for the whole population”.\nBasis for assumption: Briefly summarise why the assumption matters for the analysis and why it is reasonable. For example, the assumption could be based on historical data, theoretical requirements of the method, empirical evidence, quality assurance, common practice or data testing. Numerical value of the assumption: If you can, attach a numerical value to the assumption. This will depend on what the assumption is and how it is applied in your work. For example, you might make an assumption that counts in your data are accurate to within 5 units, so you would record +/- 5 as the numerical value. Thinking about the value of the assumption in numeric terms is a useful way to work through its impact on your work. Range around the estimated value: Assumptions are rarely certain. If you can, assign a range to the central value of the assumption. Links to supporting analysis: Attach a link to the source where the assumption comes from or from where it can be verified and justified.\nDocumentation dependencies: List the project documents dealing with this assumption. Assumptions can have an impact on different stages of the analysis. For example, an assumption about input data if valid or in-valid could impact the model code, project timeline, robustness of the outputs produced by the model. Logging these dependencies will ensure that the relevant plans and documents are updated once an assumption is validated.\nInternally reviewed by: Enter the full name of individual or group who reviewed the assumption.\nDate of last review/update: Enter the date the assumption was last reviewed or validated.\nExternally reviewed by: Enter the name of organisation and the position of the expert who validated the assumption. It is not necessary to mention the individual’s name. For example, Lighthouse Laboratory (Senior Scientist), University of Oxford (Professor of Statistics).\nDate of external review: Enter the date the assumption was reviewed by the external expert.\nNext review/update due on: Enter the date the assumption next needs to be reviewed or updated.\nQuality Rating: See the Quality Rating Key and fill it accordingly.\nRisk Score: See the Risk Score and fill it accordingly.",
    "crumbs": [
      "Assumptions and issues log guide"
    ]
  }
]