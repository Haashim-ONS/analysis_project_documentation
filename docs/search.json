[
  {
    "objectID": "quality_questions.html",
    "href": "quality_questions.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo get the most out of the template, we strongly recommend that teams identify who will take the three key quality assurance roles of Commissioner, Senior Responsible Owner and Analytical Assurer at the start of the analytical cycle. This is crucial as each of these individuals has a role in ensuring that the analysis you do is fit-for-purpose. You should also identify the members of the analytical team.",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "quality_questions.html#iv.-delivery",
    "href": "quality_questions.html#iv.-delivery",
    "title": "Quality Questions",
    "section": "IV. Delivery",
    "text": "IV. Delivery\n\nQuality QuestionsWhy the questions matterThe questions and the Code of PracticeLinking the questions to AQuA roles\n\n\n\n\n\n\n\n\nQuestion\nAnswer\nAnswer added by\nDate answer added\nIs SRO satisfied with the answer?\nNext review due on\nHas anything changed from last review?\n\n\nQ52\nCan you give a clear account of what can and cannot be inferred from the analysis?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ53\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ54\nHave you sense checked outputs with user groups and stakeholders?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ55\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ56\nAre the implications of unquantified uncertainties communicated to users?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ57\nIs workflow documentation including technical guides and code repositories publicly available?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ58\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ59\nHave you fully documented the analysis code to comply with good practice?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ60\nIs there a clear feedback mechanism so users can report back on the suitability of outputs?\nEnter your answer here...\nEnter full name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhy do I need to know the answer to this?\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nOften the aim of final output is to inform decison-making. Output might include predictions, involving lots of underlying assumptions. It is critical that you support your users to make appropriate use of outputs and understand what can and cannot be inferred. Without this, there is a major risk that users may misinterpret findings, make in appropriate comparisons, use the analysis for unsuitable purposes and arrive at the wrong conclusions. For example, a non-expert user may wrongly interpret correlation as causation or use incomplete or disconnected data to make forecasts.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nYou should describe why limitations related to data and methods exist, why they cannot be overcome using the chosen approach and their impact on the quality and interpretation of the output. Analysis is of very little value if limitations aren’t properly documented and explained.\n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nYou should work with users, experts and other relevant stakeholders to verify the credibility of outputs and sense check that they are useful.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nOutputs are never 100% accurate. Users need to understand how uncertainties related to data, assumptions and methodology feed into and through the analysis workflow and what this means for the use of the outputs. Results must clearly explain how uncertainty affects the findings from the analysis, or we risk misinterpretations and conclusions being overly reliant on imprecise results.\n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nYou must support your users so they have a sound understanding of relevant uncertainties which are not captured in the analysis. When you can, make reasonable judgements about the likely size and direction of unquantified uncertainty. Provide a qualitative description informing users about why the uncertainty cannot be quantified and their likely impact.\n\n\n\n\nQ57\n\n\nIs workflow documentation including technical guides and code repositories publicly available?\n\n\nTransparency about your analysis supports proper scrutiny and challenge, promotes public trust and encourages re-use of the resources you develop.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nA good technical guide helps everybody to understand what the analysis does and how it works. A well-written technical guide is essential for effective maintenance of the analysis over the project cycle. It helps users of the analysis to replicate the findings, get answers to methodology questions and build their trust in the output. \n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nThe technical guide is complemented by fully documented analysis code. Code documentation must comply with good practice so new users can understand and execute the code as easily and quickly as possible.\n\n\n\n\nQ60\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nExternal critique makes analysis more robust. Users should be able to give feedback to your team to ensure that results meet their needs. User feedback and customer reviews inform you of issues and changes that you might need to make. They also act as evidence that users have been consulted. \n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\nWhich pillar and principle of Code of Practice matter here?\n*Trustworthiness (T), Quality (Q), Value (V)\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely. Q1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported. Q3.1 Statistics should be produced to a level of quality that meets users’ needs. The strengths and limitations of the statistics and data should be considered in relation to different uses, and clearly explained alongside the statistics.\n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nQ3 Producers of statistics and data should explain clearly how they assure themselves that statistics and data are accurate, reliable, coherent and timely.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nQ1.5 Potential bias, uncertainty and possible distortive effects in the source data should be identified and the extent of any impact on the statistics should be clearly reported. Q2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation.\n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nQ2.4 Relevant limitations arising from the methods and their application, including bias and uncertainty, should be identified and explained to users. An indication of their likely scale and the steps taken to reduce their impact on the statistics should be included in the explanation. Q3.3 The extent and nature of any uncertainty in the estimates should be clearly explained.\n\n\n\n\nQ57\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nV2 Statistics and data should be equally available to all, not given to some people before others. They should be published at a sufficient level of detail and remain publicly available. V3 Statistics and data should be presented  clearly, explained meaningfully and provide authoritative insights that serve the public good.\n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nT4.4 Good business practices should be maintained in the use of resources. Where appropriate, statistics producers should take opportunities to share resources and collaborate to achieve common goals and produce coherent statistics. Q2.1 Methods and processes should be based on national or international good practice, scientific principles, or established professional consensus. \n\n\n\n\nQ60\n\n\nAre users able to feed back on the suitability of outputs?\n\n\nV1 Users of statistics and data should be at the centre of statistical production; their needs should be understood, their views sought and acted upon, and their use of statistics supported. V1.4 Statistics producers should engage publicly through a variety of means that are appropriate to the needs of different audiences and proportionate to the potential of the statistics to serve the public good. An open dialogue should be maintained using proactive formal and informal engagement to listen to the views of new and established contacts. Statistics producers should undertake public engagement collaboratively wherever possible, working in partnership with policy makers and other statistics producers to obtain the views of stakeholders. V1.5 The views received from users, potential users and other stakeholders should be addressed, where practicable. Statistics producers should consider whether to produce new statistics to meet identified information gaps. Feedback should be provided to them about how their needs can and cannot be met, being transparent about reasons for the decisions made and any constraints.\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Question\n\n\nWhich AQuA role(s) would normally answer this?\n\n\nWhy are these AQuA roles involved?\n\n\n\n\nQ52\n\n\nCan you give a clear account of what can and cannot be inferred from the analysis?\n\n\nCommissioner, analyst\n\n\nDuring the delivery phase, the commissioner receives the results of the analysis and decides whether it meets their needs. The analyst provides sufficient information to support the commissioner to make an informed decision.\n\n\n\n\nQ53\n\n\nHave you assessed the limitations of the data and analysis and set out how they affect the quality and use of the outputs?\n\n\nCommissioner, analytical assurer, analyst\n\n\nThe commissioner must be confident in the quality of the outputs. They should understand the strengths, limitations and context of the analysis so that the results are correctly interpreted. Analytical assurer sign-off provides confidence that analysis risks, limitations and major assumptions are understood by the users of the analysis. Analysts make sure that the commissioner and analytical assurer have the evidence they need. \n\n\n\n\nQ54\n\n\nHave you sense checked outputs with user groups and stakeholders?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst and analytical assurer should enable and encourage peer review. Peer reviews provide useful critical challenge about the analytical approach, application of methods and interpretation of the analysis. Verification and peer review of work should be done by analysts who had no involvement in the work  so their views are independent.\n\n\n\n\nQ55\n\n\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\n\n\nAnalyst, commissioner\n\n\nThe analyst must determine and communicate the uncertainty associated with the analysis so the commissioner can make informed decisions. The commissioner should ensure that an assessment of uncertainty has been provided and that the implications of uncertainty are understood. \n\n\n\n\nQ56\n\n\nAre the implications of unquantified uncertainties communicated to users?\n\n\nAnalyst, commissioner\n\n\nIf uncertainty is too complex to quantify, even approximately, the analysts should explain this so the commissioner can take this into account. In communicating analysis results to decision-makers and stakeholders, the commissioner should be open about the existence of deep uncertainties whose impact cannot be assessed, and explain how they are managed in the analysis.\n\n\n\n\nQ57\n\n\nIs documentation including technical guides and code repositories publicly available?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate design documentation. Best practice includes maintaining a record of the analysis workflow in a technical report, including a concept of analysis, user requirements, design specification, functional specification, data dictionary, and test plan. Code should be properly documented.\n\n\n\n\nQ58\n\n\nDoes the technical guide and documentation explain how to run the analysis to obtain valid outputs?\n\n\nAnalyst, analytical assurer\n\n\nThe analyst must produce appropriate documentation. Best practice includes maintaining a record of the work that has been done in a technical report, including a full description of the analysis, user requirements, design specification, functional specification, data dictionary, and test plan. The analytical assurer makes sure that the documentation is fit for purpose.\n\n\n\n\nQ59\n\n\nHave you fully documented the analysis code to comply with good practice?\n\n\nAnalyst, analytical assurer\n\n\nAnalysts should develop and maintain analysis code in line with best practice. Code must comply with relevant policies and standards.\n\n\n\n\nQ60\n\n\nIs there a clear feedback mechanism so users can report back on the suitability of outputs?\n\n\nAnalyst, senior responsible owner\n\n\nYou can assess the usefulness of the analysis by getting feedback from users, stakeholders and other experts. Quality analysis should be free of prejudice or bias. The SRO and analysts should check that the analysis follows the principles of RIGOUR (Repeatable, Independent, Grounded in reality, Objective, Uncertainty-managed, Robust)",
    "crumbs": [
      "Quality Questions"
    ]
  },
  {
    "objectID": "sample_assumptions_log.html",
    "href": "sample_assumptions_log.html",
    "title": "Assumptions Log",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Sample assumptions log"
    ]
  },
  {
    "objectID": "sample_assumptions_log.html#definitions",
    "href": "sample_assumptions_log.html#definitions",
    "title": "Assumptions Log",
    "section": "Definitions",
    "text": "Definitions\nAssumptions are ranked red, amber or green (a RAG score) depending on quality. The quality of an assumption measures both how certain and robust an assumption is and how appropriate it is for its intended use.\nFor example, we would usually consider a well documented assumption drawn from published evidence to be very robust, but if it needs to be transformed or adapted significantly to fit the analysis, the quality rating would probably need downgrading. You would normally lower the quality rating if it is not possible to get technical sign-off for the assumption (e.g. because of lack of technical knowledge). You will also want to lower the quality if the confidence interval is likely to be wide (i.e. you wouldn’t be surprised if the value was 50% different from what you measure because of uncertainty).\n\n\n\n\n\n\n\nRAG Rating\nAssumption quality\n\n\n\n\nGREEN\nBased on validated “actual” data; Methodology is robust; No or few transformations, or transformation methodology is fully verified and robust; Data is current and signed off by experts; Confidence intervals are narrow.\n\n\nAMBER\nThe methodology is robust but based on limited data; Data required significant transformation to fit the model; Confidence interval is quite wide; Data has not been reviewed recently.\n\n\nRED\nUnclear/unreliable data source or no data source provided; Based on limited data and methodology not robust; Data is not current; Confidence interval is wide or quality is unknown.",
    "crumbs": [
      "Sample assumptions log"
    ]
  },
  {
    "objectID": "sample_assumptions_log.html#assumptions---uk-universities-input-dataset",
    "href": "sample_assumptions_log.html#assumptions---uk-universities-input-dataset",
    "title": "Assumptions Log",
    "section": "Assumptions - UK universities input dataset",
    "text": "Assumptions - UK universities input dataset\nThis analysis makes the following assumptions\n\n\n\n\n\n\n\n\nAssumption Number\nAssumption description\nQuality Rating\n\n\n\n\n1\nData is representative of the population\nGREEN\n\n\n2\nData does not exclude any population groups based on their demographic and socio-economic characteristics\nGREEN\n\n\n3\nAll UK universities report data to the Higher Education Statistics Agency (HESA)\nRED\n\n\n4\nAll universities accurately report the number of students enrolled during the academic year\nRED\n\n\n5\nAll universities accurately report the number of students who dropped out during the academic year\nRED\n\n\n6\nThe academic year is consistently measured across UK universities\nRED\n\n\n7\nStudents who receive special education services are excluded from the calculation of dropout rates\nRED\n\n\n8\nThere is complete information for all the variables in the analysis\nRED\n\n\n9\nThe data collection process has not changed at all over time\nRED",
    "crumbs": [
      "Sample assumptions log"
    ]
  },
  {
    "objectID": "issues_log.html",
    "href": "issues_log.html",
    "title": "Issues log",
    "section": "",
    "text": "The issues log provides a standard template to record all the issues incurred during the analysis lifecycle. Without documentation, risks and issues may be well understood by one part of the team and totally unknown by others. It is therefore important to maintain an issues log to ensure that everybody knows about the issues that the analysis includes. The register helps teams to store problems and challenges that arise during analysis development for future review and mitigation. It aims to make the system of analysis development more efficient and more effective and prevent the retreading of old ground.\n\n\nStart filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis publications or output report) and financial year of interest.\nIssue ID: Give each issue a unique ID so that it can be tracked easily and cross referenced.\nIssue: Enter a name for the issue. It could be anything raised by the team, anything raised during quality assurance by the team members or external reviewers. Include the location of the issue, for example, line number of code, error in publication, where it arises in the workflow, resource constraint.\nDate first identified: Enter the date the issue was first identified.\nPlain English description of issue: A brief summary of the underlying cause and nature of issue in plain English explaining what is creating problem.\nImpact of issue: Brief summary of what is being impacted, for example, timeline, accuracy, cost of project.\nStatus of issue: From the dropdown menu, select ‘Resolved’ if the issue has been resolved or ‘Shelved’ if the issue has yet to be handled.\nJustification of status: Brief summary of how the proposed solution has helped in overcoming the issue if the status is ‘Resolved’. For ‘Shelved’ issues, explain why it is decided to deal with later.\nProof of resolution: Write the name of the output report/methodology paper or the published document which the issue relates to and how the resolution has impacted it. If the issue was related to code, identify the location of revision made within the code by mentioning the line number.\nDate of last review/update: Enter the date the issue was reviewed/updated.\nReviewed by: Enter the full name of individual(s) within the analysis team who have reviewed the decision made to resolve or shelve the issue.\nNext review/update due on: Enter the date the issue needs to be reviewed or updated in future.",
    "crumbs": [
      "Issues log guide"
    ]
  },
  {
    "objectID": "issues_log.html#definitions",
    "href": "issues_log.html#definitions",
    "title": "Issues log",
    "section": "",
    "text": "Start filling in the template by inserting the full name of the analysis (this should align with the name given on the analysis publications or output report) and financial year of interest.\nIssue ID: Give each issue a unique ID so that it can be tracked easily and cross referenced.\nIssue: Enter a name for the issue. It could be anything raised by the team, anything raised during quality assurance by the team members or external reviewers. Include the location of the issue, for example, line number of code, error in publication, where it arises in the workflow, resource constraint.\nDate first identified: Enter the date the issue was first identified.\nPlain English description of issue: A brief summary of the underlying cause and nature of issue in plain English explaining what is creating problem.\nImpact of issue: Brief summary of what is being impacted, for example, timeline, accuracy, cost of project.\nStatus of issue: From the dropdown menu, select ‘Resolved’ if the issue has been resolved or ‘Shelved’ if the issue has yet to be handled.\nJustification of status: Brief summary of how the proposed solution has helped in overcoming the issue if the status is ‘Resolved’. For ‘Shelved’ issues, explain why it is decided to deal with later.\nProof of resolution: Write the name of the output report/methodology paper or the published document which the issue relates to and how the resolution has impacted it. If the issue was related to code, identify the location of revision made within the code by mentioning the line number.\nDate of last review/update: Enter the date the issue was reviewed/updated.\nReviewed by: Enter the full name of individual(s) within the analysis team who have reviewed the decision made to resolve or shelve the issue.\nNext review/update due on: Enter the date the issue needs to be reviewed or updated in future.",
    "crumbs": [
      "Issues log guide"
    ]
  },
  {
    "objectID": "faqs.html",
    "href": "faqs.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#when-should-i-use-this-guidance",
    "href": "faqs.html#when-should-i-use-this-guidance",
    "title": "Frequently Asked Questions",
    "section": "When should I use this guidance?",
    "text": "When should I use this guidance?\nWe encourage all teams starting work on a new analysis project to use this guidance right from the scoping stage. For teams who are in the middle of an analysis, you can still use the guidance and templates to reflect on the quality questions from the scoping and design stages and continue recording the answers up until the delivery stage.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "href": "faqs.html#which-analysis-is-in-scope-of-the-guidance",
    "title": "Frequently Asked Questions",
    "section": "Which analysis is in scope of the guidance?",
    "text": "Which analysis is in scope of the guidance?\nThe guidance applies to all analysis workflows, including analysis to support research, advice for other departments, ad-hoc and one-off outputs and regular outputs.\nRegular outputs often fulfil at least one of the criteria for what makes a business critical model: 1. Drives essential financial and funding decisions; 2. Essential to the achievement of business plan actions and priorities; 3. Must be error free or else risk serious financial or legal penalties or reputational damage for the organisation.\nBusiness critical analysis requires the highest level of scrutiny and the governance arrangements must be appropriate for the level of risk.\nFor ad-hoc outputs with quick turnarounds and limited time and resources, analysis still needs sufficient quality assurance to ensure it is fit for purpose. Ad-hoc outputs are often used as part of the evidence when making important policy decisions, so getting them right is very important.\nWe strongly encourage senior responsible owners of both regular and ad-hoc analysis to answer all questions relevant to their role. Completing the logs will make it much easier to understand the risks that the analysis carries and help you plan to mitigate them. It will also be useful when you need to explain why the analysis works as it does, to raise quality and resourcing issues, push back against demands that are unrealistic or unachievable and to understand and communicate potential risks right from the scoping stage.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "href": "faqs.html#how-should-i-identify-the-senior-responsible-owner-commissioner-analyst-and-analytical-assurer",
    "title": "Frequently Asked Questions",
    "section": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?",
    "text": "How should I identify the Senior Responsible Owner, Commissioner, Analyst and Analytical Assurer?\nThe AQuA Book sets out four roles responsible for the assurance of analysis:\n* Commissioner * Senior Responsible Owner (SROs)\n* Analyst teams (who usually report to the SRO) and\n* Analytical Assurer.\nWhat matters here is making sure that each assurance role is covered in your workflow and that individuals know about and accept their responsibilities, not the name of the role.\nAs a project team, you should make sure that each role is in place and you understand how it will operate so that you have the right assurance. The AQuA Book makes no expectations about levels of seniority or grade of each of the occupiers of the roles. It does not specify whether roles should be held by a person or could be held by a committee or other governance group (such as a senior leadership team). The key consideration is whether or not the person or group that undertakes the assurance role have the skills and resources they need to meet the requirements of the role. How the roles are covered in a particular analysis workflow will vary from project to project, depending on how the work is planned and assured.\nWe suggest that the SRO and commissioner should usually be at Grade 7 or above. If you are still unsure about how to allocate the roles among your team and governance groups, please email us with “analysis assurance” in the subject header and we will help you to make the decision.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "href": "faqs.html#what-help-is-available-to-answer-the-quality-questions",
    "title": "Frequently Asked Questions",
    "section": "What help is available to answer the quality questions?",
    "text": "What help is available to answer the quality questions?\nThe ONS Quality Central wiki contains lots of useful guidance, templates and mandatory training to help you work through and answer the quality questions. It inclu des the ONS Quality Standard for Analysis.\nThese resources are also likely to be useful:\n* Government Data Quality Hub Quality Questions and Red Flags and Government Data Quality Framework * Office for Statistics Regulation Quality Assurance of Administrative Data toolkit",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "assumptions_log.html",
    "href": "assumptions_log.html",
    "title": "Assumptions Log",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Assumptions log guide"
    ]
  },
  {
    "objectID": "assumptions_log.html#definitions",
    "href": "assumptions_log.html#definitions",
    "title": "Assumptions Log",
    "section": "Definitions",
    "text": "Definitions\nStart filling the template by inserting the full name of the analysis (this should align with the name given on the analysis output report) and financial year of interest.\nAssumption ID: Give each assumption a unique ID so that it can be tracked easily and cross referenced.\nLocation in code/publication: Write the name of the output report/methodology paper or the published document in which the assumption has been mentioned. If the assumption is related to code, identify where the assumption applies within the code by mentioning the line number.\nPlain English description of assumption: Write a clear description of the assumption in plain English. For example, ‘it is assumed that data is representative for the population’.\nBasis for assumption: Briefly summarise the basis of assumption to justify their suitability given the modelling scenario. For instance, the assumption could be based on historic data, theoretical literature, empirical evidence, quality assurance reports, data testing or robustness checks performed by the team. Links to supporting analysis: Attach a link of the source where the assumption has been taken from or from where it can be verified.\nDocumentation dependencies: List down the name of project documents dealing with this assumption. Assumptions can have an impact on different stages of the modelling cycle. For example, an assumption about input data if valid or in-valid could impact the model code, project timeline, robustness of the outputs produced by the model. Logging these dependencies will ensure that the relevant plans and documents are updated once an assumption is validated.\nInternally reviewed by: Enter the full name of individual within the analysis team responsible for ensuring that the assumption is validated appropriately.\nDate of last review/update: Enter the date the assumption was last reviewed or validated.\nExternally reviewed by: Enter the name of organisation and the position of expert who has validated the assumption. It is not necessary to mention the individual’s name. For example, Lighthouse Laboratory (Senior Scientist), University of Oxford (Professor of Statistics).\nDate of external review: Enter the date the assumption was reviewed by the external expert.\nNext review/update due on: Enter the date the assumption needs to be reviewed or updated.\nQuality Rating: See the Quality Rating Key and fill it accordingly.\nRisk Score: See the Risk Score and fill it accordingly.",
    "crumbs": [
      "Assumptions log guide"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\n\n\n\n\nFeedback\nWe are keen to understand the challenges faced by analytical teams in ONS when complying with good practice. To evaluate the effectiveness of this guidance, we would be grateful if you could fill in a short survey.\nWe appreciate your feedback. It will help us to make additional tools and resources on good practice for producing quality analysis.\nIf you have questions about this guidance or the survey please email ASAP@ons.gov.uk with “analytical QA” in the subject header.",
    "crumbs": [
      "Feedback"
    ]
  },
  {
    "objectID": "quality_questions_markdown_version.html",
    "href": "quality_questions_markdown_version.html",
    "title": "Quality Questions",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.\nTo make the most out of this template, we strongly recommend that you identify who will take the three key quality assurance roles of Commissioner, Senior Responsible Owner and Analytical Assurer at the start of the analytical cycle. This is crucial as each of these individuals has a role in ensuring that the analysis you do is fit-for-purpose. You should also identify the members of the analytical team. If team members have specific roles, record them here and update them when they change."
  },
  {
    "objectID": "quality_questions_markdown_version.html#lead-analytical-roles",
    "href": "quality_questions_markdown_version.html#lead-analytical-roles",
    "title": "Quality Questions",
    "section": "Lead analytical roles",
    "text": "Lead analytical roles\n\n\n\n\n\n\n\n\n\nName of Senior Responsible Owner (SRO)\nName of Commissioner\nName of Analyst(s)\nName of Analytical Assurer(s)\n\n\n\n\nEnter name here\nEnter name here\nEnter name here\nEnter name here"
  },
  {
    "objectID": "quality_questions_markdown_version.html#quality-questions",
    "href": "quality_questions_markdown_version.html#quality-questions",
    "title": "Quality Questions",
    "section": "Quality Questions",
    "text": "Quality Questions\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoping\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ1\nWhat question is the analysis trying to answer?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ2\nWhy do you need to answer this analysis question?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ3\nWhich organisational priorities does this analysis address?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ4\nIf you use a model, is it business critical?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ5\nWho needs the answer to the analysis question?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ6\nWho do you need to consult to make sure you meet the right user needs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\n\n\n\nQ7\nHow will you know you have answered the analysis question correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\n\n\n\nQ8\nWhat is the estimated time and resource required to answer the analysis question (in months and FTE)?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ9\nWhat is the impact if the analysis is not done now?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ10\nWhat is the impact if the work is not done correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ11\nName the commissioner, senior responsible owner and analytical assurer of this analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ12\nWhat tools and resources will you use in production? Are they the best for the job?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ13\nDo you have the right internal and external resources and capability to deliver the analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ14\nWhat are the anticipated risks of the analysis? Have you discussed these risks with customers and stakeholders?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ15\nIs there a contingency plan prepared if your mitigation plans fail?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ16\nDo the data and analysis comply with ethical requirements?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ17\nWhat relevant questions are outside the scope of the analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ18\nHow will you peer review and assure the work?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ19\nWill external experts be involved in development and scrutiny of analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesign\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ20\nIs there a simple description in plain English of what the analysis is for and what it does?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ21\nDoes the analysis have a logic flowchart which explains the end-to-end conceptual steps in the work flow?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ22\nWhen do you expect to start and finish each stage of analysis i.e., data collection, processing, quality assuarnce, analaysis and dissemination?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ23\nDoes any part of the analysis rely on manual processing? Have you considered the cost and benefits of fully automating the process?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ24\nWhat happens if any of your team members, reviewers or users find a mistake in your analysis? Do you have a clear and efficient process for addressing the concern and preventing it from happening again?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ25\nHave you assessed uncertainty?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoing and checking the analysis\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ26\nHow the data used in the analysis will be processed prior to and during use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ27\nIs the data appropriate given the methods selected?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ28\nWhat are the strengths and limitations of the data that you use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ29\nIs there a robust relationship between your team and data providers? Does your data provider have a good understanding of how and why you are using their data? Do you have a good understanding of how the data provider collects, processes and quality assures the data?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ30\nIs there a formal agreement in place that specifies when, what and how the data will be received? If not, would this be helpful?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ31\nDo you know what quality checks are carried out on the data before you receive them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ32\nHow will you work with your data provider when your data requirements change?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ33\nHow do you know if your data provider makes a change to their systems or processes, which could impact the data you receive and/or the statistics you produce?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ34\nHow did you choose the methods for the analysis? How do you know the method you are using is appropriate?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ35\nHave reasonable alternative methods been explored and rejected for good reasons?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ36\nHow do you know that your analysis works correctly?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ37\nCan you describe the assumptions of your analysis, when they were made and who made them and signed them off?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ38\nHow are your assumptions validated and assured prior to use?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ39\nHow do you measure and report uncertainty in your analysis?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ40\nHave you thought about the implications of any unquantified uncertainties?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ41\nCan you explain how your analysis feeds into downstream processes? Are there any risks around these dependencies?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ42\nIs all or part of the analysis reliant on a single person? If yes, how are you mitigating this risk?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ43\nIs it clear why important decisions were made and who made them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ44\nIf changes are made to code or datasets, can you easily track who made the changes and why the changes were made?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ45\nCould another analyst carry out the analysis and make the outputs just by reading documentation and desk instructions without needing to consult with anybody else?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ46\nDo you use peer review to check scripts and code, documentation, implementation of methods, processes and outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ47\nIs your code and analysis ever peer reviewed by someone outside your team or organisation?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ48\nWhat is your assessment of the quality of your analytical outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ49\nHow do you assure yourselves that analysis carried out is correct?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ50\nDo the outputs of your analysis match reported content?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ51\nIf you find outliers or unusual trends in the data, what steps are taken to investigate them?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelivery\nQuality question\nAnswer\nAnswer added by\nDate of answer\nAnswer signed off by SRO?\nNext review due date\nChanges since last review\n\n\n\n\nQ52\nCan you give a clear account of what can and cannot be inferred from your final output? Q52 Could you give a clear account of what can and cannot be inferred from your final output?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ53\nHave you assessed the impact of the data and analysis limitations and set out how they will affect the quality and use of the outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ54\nHave you sense checked outputs with user groups and stakeholders?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ55\nIs uncertainty about data quality, assumptions and methodology clearly communicated to users?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ56\nAre the implications of unquantified uncertainties communicated to users?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ57\nIs analysis documentation, including technical guides and code, publicly available?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ58\nDoes the technical guide explain how to use the analysis to obtain valid outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ59\nHave you fully documented the analysis code?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text\n\n\nQ60\nAre users able to feed back on the suitability of outputs?\nEnter your answer here…\nEnter name\nEnter date\nYes/No\nEnter next review date\nEnter text"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quality Questions Introduction",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis guidance is an ALPHA draft. It is in development and we are still working to ensure that it meets user needs.\nPlease get in touch with feedback to support the guidance by creating a GitHub Issue or emailing us.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Quality Questions Introduction",
    "section": "Aims",
    "text": "Aims\nThis guidance provides a set of questions to help analytical and statistical teams evaluate the quality of their analysis throughout the production cycle. The guidance is here to support teams in meeting the Office for National Statistics’s (ONS) strategic objectives for improving statistical quality. You can find more information about our strategic objectives on statistical quality in the ONS Statistical Quality Improvement Strategy. ONS manages quality through a strategic risk approach.\nThe guidance has five main aims:\n\nTo help analysts understand the level of risk they are carrying in their analytical workflows.\nTo ensure there is a consistent end-to-end QA approach across ONS.\n\nTo make it easier to comply with good practice guidance and standards including the ONS Quality Practices, ONS Quality Standard for Analysis, the government AQUA Book and the Code of Practice for Statistics, the Analysis Function Functional Standard for Analysis and the Government Service Manual which explains how to research, document and validate user needs.\n\nTo ensure there is a consistent understanding of roles and responsibilities when producing high quality analysis and statistics.\n\nTo make it easier to create critical project documentation including an assumptions and decisions log, issue and decisions log, risk register and divisional Quality Improvement Plan.\n\nReflecting on the questions asked in this template will help you to manage your analysis risks:\n\nYou will be able to document the mitigation that is in place or planned.\n\nYou will know that the project is prepared to accept issues or risks and why this is.\n\nYou can identify potential quality issues and decide how to manage and prioritise them.\n\nHaving this information in once place provides a sound basis for regular reviews of assumptions, issues and risks associated with the workflow, in line with recommended good practice.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-the-questions-draw-on-other-frameworks",
    "href": "index.html#how-the-questions-draw-on-other-frameworks",
    "title": "Quality Questions Introduction",
    "section": "How the questions draw on other frameworks",
    "text": "How the questions draw on other frameworks\nThe AQuA book sets out a standard framework for managing analytical quality in government. The AQuA framework is there to make sure that our work can be trusted to inform good decision making, while the Code of Practice for Statistics sets out the principles and practices that producers of official statistics should commit to.\nTwo other pieces of guidance have motivated us to produce this template. One is the Analysis Function guidance on Quality Questions and Red Flags. The other is the Office for Statistics Regulation (OSR) guidance on Thinking about quality when producing statistics. Both of these provide sets of questions that analysts can use to interrogate their work and assure its quality.\nBuilding on these cross-government resources, this guidance sets out quality questions that are relevant for each stage of analytical cycle. The quality questions are at their most effective if they are asked at the right stage. Once that stage is passed, experience suggests that it is normally difficult to go back and address the points the questions ask by retrofitting at a later stage of the analysis.\nEach question is linked with the Code of Practice for Statistics pillars of Trustworthiness, Quality and Value. We explain the importance and relevance of each question in light of the three pillars so teams can better understand and apply these principles through out the project life cycle.\nQuality questions are also categorised by which responsible role from the AQuA book would usually answer them. The idea is to highlight the clear line of accountability set out in the Aqua book in an easy-to-understand manner. We want to make it easier for teams to decide how the three key Aqua roles of commissioner, senior responsible owner (and their analysis team) and analytical assurer are covered in their own workflows.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-guidance",
    "href": "index.html#structure-of-the-guidance",
    "title": "Quality Questions Introduction",
    "section": "Structure of the guidance",
    "text": "Structure of the guidance\nThere are 60 quality questions in total. They cover all the stages of the analytical cycle.\nAnswering 60 questions may seem a daunting task at first! The questions are designed to help you as you work your way through the analytical process, rather than to be answered all at once. Creating a log of answers as you move through the stages of the analysis workflow will help you to check that your work meets analytical standards, follows good practice and (if relevant) complies with the Code of Practice for Statistics.\nAnswering the questions will also help you make sure that everybody working on the analysis has a clear understanding of how and why it works as it does, and to support your users when writing your outputs. Moreover, most of the answers will help you to produce the critical documents that mitigate risk like an assumptions log, decisions log, issues log, and technical guides for your team and your users.\nThe AQuA book divides the stages of the analytical cycle into:\n\nScoping\n\nDesign\n\nConducting and checking analysis\n\nDelivery\n\nThe stages are covered by the following tabs on this website:\n\na) Quality Questions\nThis tab lists the quality questions that are relevant for each stage of analytical cycle. It provides space in the form of editable text boxes (with no word limit) to write the answers. We encourage commissioner, analyst team and analytical assurer to answer all the questions given their responsibilities.\nUser needs or project aims often change over time, so the template allows you to edit answers by simply changing the text in the editable text boxes. To track these changes, the person writing the answer should include their full-name, the date they have added the answer, and the date the next review is due.\nThe Senior responsible owner (SRO) is the lead analyst responsible for the workflow (or relevant part of it). The SRO needs to sign-off all the answers. It is best to complete and save the log of answers for each stage of analytical cycle. Answers can be saved by pressing Ctrl+S. To undo a action press Ctrl+Z. Pressing Ctrl+S will automatically download a copy of the HTML page on your machine.\n\n\nb) Importance\nThis tab explains why understanding each quality question matters and explains the potential risks and benefits around it. It tries to address the valid question of “why should I care about this?”.\n\n\nc) Code of Practice for Statistics\nThis tab shows the pillars and principles of Code of Practice for Statistics each quality question relates to. The text in this column is copied from Code of Practice for Statistics. Even if your work does not directly feed into the production of official statistics, compliance with the principles and practices of the Code is a good way to strengthen the resilience of your work, increase transparency and clarity and reduce risk.\n\n\nd) AQuA roles and responsibilities\nThis tab explains the responsibilities required to deliver analysis that is fit-for-purpose. The roles and responsibilities in this tab are reproduced from AQuA book. The tab links the quality questions with the relevant AQuA Book responsibilities at each stage of analytical cycle.\nThe AQuA book recommends the creation of four identifiable roles to cover different areas of responsibility: * Commissioner * Senior Responsible Owner * Analyst * Analytical assurer\nResponsibilities of the Senior Responsible Owner\nThe Senior Responsible Owner (SRO) is accountable for the analytical workflow throughout its lifecycle. The Senior Responsible Owner is usually a senior member of the analytical team, and works closely with (or manages) the Analyst role. There is no minimum grade for the SRO role, but they should have the expertise, resources and accountability to ensure that the analysis is well designed, complies with relevant standards, works as intended and is fit for purpose. - Signs off all important decisions made about the analysis to ensure that it is fit-for-purpose, prior to use.\nResponsibilities of Commissioner The Commissioner\n- Ensures that context is understood so quality assurance is appropriate and proportionate.\n- Ensures that there is enough time and resource for required assurance and account for risk.\n- Must understand strengths and limitations including uncertainty so results are interpreted correctly.\n- Delivers QUALITY OF OUTCOME (“The analysis meets user needs and we understand its limitations”).\nResponsibilities of Analyst\n- Assist the Commissioner in framing the question to ensure the right analysis is done.\n- Manage external specialists.\n- Design, build, document and run the analysis.\n- Delivers QUALITY OF CONTENT (“The analysis is well-designed and users the right tools and methods to meet user needs”).\nResponsibilities of Analytical Assurer\n- Ensures that the evidence is there to demonstrate that appropriate quality assurance has happened and that uncertainty is communicated appropriately.\n- Advises the Commissioner on whether appropriate QA has happened and about any outstanding risks.\n- Involved throughout from design through to use.\n- Delivers QUALITY OF PROCESS (“The analysis does what it is supposed to do and we can prove it”).",
    "crumbs": [
      "Introduction"
    ]
  }
]